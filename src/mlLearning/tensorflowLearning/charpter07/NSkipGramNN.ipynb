{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import string\n",
    "import requests\n",
    "import collections\n",
    "import io\n",
    "import tarfile\n",
    "import urllib.request\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "sess = tf.Session()\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "# define hyper paramaters\n",
    "\n",
    "batch_size=50  #一个训练批次的大小\n",
    "embedding_size=200 #将word 映射成1*200 的vector\n",
    "vocabulary_size=1000 #词汇库大小\n",
    "generations=50000    #train iteration\n",
    "print_loss_every=500  #每500次打印一次loss\n",
    "num_sampled=int(batch_size/2) #\n",
    "window_size=2  #n-skipGram n=2\n",
    "stops=stopwords.words(\"english\")  #不具备有用信息的词汇\n",
    "print_valid_every = 2000\n",
    "valid_words = [ 'love', 'hate', 'silly', 'sad']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "'its scenes and sensibility are all more than familiar , but it exudes a kind of nostalgic spy-movie charm and , at the same time , is so fresh and free of the usual thriller nonsense that it all seems to be happening for the first time . \\n'"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 40
    }
   ],
   "source": [
    "def load_movie_data():\n",
    "  pos_data =list(open('./temp/rt-polarity.pos','r',encoding='Windows-1252').readlines())\n",
    "  neg_data = list(open(\"./temp/rt-polarity.neg\",'r',encoding='Windows-1252').readlines())\n",
    "  texts = pos_data + neg_data\n",
    "  target = [1]*len(pos_data) + [0]*len(neg_data)     \n",
    "  return texts,target        \n",
    "texts, target = load_movie_data()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "def normalize_text(texts, stops):\n",
    " texts = [x.lower() for x in texts]\n",
    " texts = [''.join(c for c in x if c not in string.punctuation) for x in texts]\n",
    " texts = [''.join(c for c in x if c not in '0123456789') for x in texts]\n",
    " texts = [' '.join([word for word in x.split() if word not in (stops)]) for x in texts]\n",
    " texts = [' '.join(x.split()) for x in texts]\n",
    " return(texts)\n",
    "texts = normalize_text(texts, stops)\n",
    "\n",
    "#去除所有长度小于2的sentence\n",
    "target = [target[ix] for ix, x in enumerate(texts) if len(x.split()) > 2]\n",
    "texts = [x for x in texts if len(x.split()) > 2]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "def build_dictionary(sentences, vocabulary_size):\n",
    "    split_sentences = [s.split() for s in sentences] \n",
    "    words = [x  for sublist in split_sentences for x in sublist]\n",
    "    count = [['RARE', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size-1)) \n",
    "    #找到出现频率为top （vocabulary_size） 单词\n",
    "    word_dict = {} #字典，单词出现的index\n",
    "    for word, word_count in count:\n",
    "      word_dict[word] = len(word_dict)\n",
    "    return(word_dict)\n",
    "word_dictionary = build_dictionary(texts, vocabulary_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "#sentence=>[indexs of word]\n",
    "def text_to_numbers(sentences, word_dict):\n",
    " # Initialize the returned data\n",
    " data = []\n",
    " for sentence in sentences:\n",
    "   sentence_data = []\n",
    "   # For each word, either use selected index or rare word index\n",
    "   for word in sentence:\n",
    "     if word in word_dict:\n",
    "        word_ix = word_dict[word]\n",
    "     else:\n",
    "        word_ix = 0\n",
    "     sentence_data.append(word_ix)\n",
    "   data.append(sentence_data)\n",
    " return(data)\n",
    "split_sentences = [s.split() for s in texts] \n",
    "text_data = text_to_numbers(split_sentences, word_dictionary)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "#cliche 不在vocabulary 中\n",
    "valid_examples = [word_dictionary[x]  if x  in word_dictionary.keys() else 0 for x in valid_words]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "def generate_batch_data(sentences, batch_size, window_size):\n",
    " batch_data = []\n",
    " label_data = []\n",
    " while len(batch_data) < batch_size:\n",
    "   rand_sentence = np.random.choice(sentences)\n",
    "\n",
    "   window_sequences = [\n",
    "         rand_sentence[max((ix-window_size),0):(ix+window_size+1)]\n",
    "         for ix, x in enumerate(rand_sentence)\n",
    "                      ] \n",
    "    \n",
    "   label_indices = [ix if ix<window_size else window_size \n",
    "                  for ix,x in enumerate(window_sequences)]\n",
    "\n",
    "   batch_and_labels = [\n",
    "       (x[y], x[:y] + x[(y+1):]) \n",
    "       for x,y in zip(window_sequences, label_indices)]\n",
    "\n",
    "   tuple_data = [(x, y_) for x,y in batch_and_labels for y_ in y]\n",
    "   \n",
    "   batch, labels = [list(x) for x in zip(*tuple_data)]\n",
    "   #zip(*tuple_data) \n",
    "   batch_data.extend(batch[:batch_size])  #装入bacth中\n",
    "   label_data.extend(labels[:batch_size]) \n",
    "   \n",
    " batch_data = batch_data[:batch_size] #再次整理\n",
    " label_data = label_data[:batch_size]\n",
    " \n",
    " batch_data = np.array(batch_data)\n",
    " label_data = np.transpose(np.array([label_data]))\n",
    " return batch_data,label_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "#define model\n",
    "\n",
    "embeddings=tf.Variable(tf.random_uniform([vocabulary_size,embedding_size],-1.0,1))\n",
    "x_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "y_target = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "embed = tf.nn.embedding_lookup(embeddings, x_inputs)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "#nce loss\n",
    "nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / np.sqrt(embedding_size)),dtype=tf.float32)\n",
    "nce_biases = tf.Variable(tf.zeros([vocabulary_size]),dtype=tf.float32)\n",
    "loss=tf.reduce_mean(tf.nn.nce_loss(\n",
    "    weights=nce_weights,\n",
    "    biases=nce_biases,\n",
    "    inputs=embed,\n",
    "    labels=y_target,\n",
    "    num_sampled=num_sampled,\n",
    "    num_classes=vocabulary_size\n",
    "))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "#通过计算余弦相似度来验证embedding后的vector\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "normalized_embeddings = embeddings / norm #标准化embedding\n",
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss)\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Loss at step 500 : 3.081111431121826\n",
      "Loss at step 1000 : 4.658157825469971\n",
      "Loss at step 1500 : 3.3214972019195557\n",
      "Loss at step 2000 : 3.5877716541290283\n",
      "Loss at step 2500 : 3.472334623336792\n",
      "Loss at step 3000 : 4.921047687530518\n",
      "Loss at step 3500 : 3.141298770904541\n",
      "Loss at step 4000 : 4.358353614807129\n",
      "Loss at step 4500 : 3.316549301147461\n",
      "Loss at step 5000 : 4.208366870880127\n",
      "Loss at step 5500 : 4.06997013092041\n",
      "Loss at step 6000 : 2.798509120941162\n",
      "Loss at step 6500 : 3.2893781661987305\n",
      "Loss at step 7000 : 4.089358806610107\n",
      "Loss at step 7500 : 5.819401741027832\n",
      "Loss at step 8000 : 3.426023244857788\n",
      "Loss at step 8500 : 3.009683132171631\n",
      "Loss at step 9000 : 3.3406569957733154\n",
      "Loss at step 9500 : 2.933220148086548\n",
      "Loss at step 10000 : 3.3348610401153564\n",
      "Loss at step 10500 : 3.4028470516204834\n",
      "Loss at step 11000 : 3.164668083190918\n",
      "Loss at step 11500 : 3.6683080196380615\n",
      "Loss at step 12000 : 2.7962236404418945\n",
      "Loss at step 12500 : 3.6083927154541016\n",
      "Loss at step 13000 : 2.8174185752868652\n",
      "Loss at step 13500 : 4.1081953048706055\n",
      "Loss at step 14000 : 3.284362554550171\n",
      "Loss at step 14500 : 2.7290396690368652\n",
      "Loss at step 15000 : 3.5557918548583984\n",
      "Loss at step 15500 : 3.1189093589782715\n",
      "Loss at step 16000 : 3.073199987411499\n",
      "Loss at step 16500 : 3.3165442943573\n",
      "Loss at step 17000 : 3.0561461448669434\n",
      "Loss at step 17500 : 3.087895393371582\n",
      "Loss at step 18000 : 2.1102397441864014\n",
      "Loss at step 18500 : 3.1761815547943115\n",
      "Loss at step 19000 : 5.284931659698486\n",
      "Loss at step 19500 : 3.2729969024658203\n",
      "Loss at step 20000 : 2.411834955215454\n",
      "Loss at step 20500 : 3.415323495864868\n",
      "Loss at step 21000 : 2.736008644104004\n",
      "Loss at step 21500 : 3.2301323413848877\n",
      "Loss at step 22000 : 2.928969144821167\n",
      "Loss at step 22500 : 3.0666110515594482\n",
      "Loss at step 23000 : 2.801370859146118\n",
      "Loss at step 23500 : 3.0805916786193848\n",
      "Loss at step 24000 : 2.697913885116577\n",
      "Loss at step 24500 : 2.4700684547424316\n",
      "Loss at step 25000 : 3.1628289222717285\n",
      "Loss at step 25500 : 3.5904624462127686\n",
      "Loss at step 26000 : 3.28031063079834\n",
      "Loss at step 26500 : 3.519721031188965\n",
      "Loss at step 27000 : 2.6335325241088867\n",
      "Loss at step 27500 : 3.944995164871216\n",
      "Loss at step 28000 : 3.124859571456909\n",
      "Loss at step 28500 : 3.467067241668701\n",
      "Loss at step 29000 : 3.8549585342407227\n",
      "Loss at step 29500 : 3.036031723022461\n",
      "Loss at step 30000 : 3.0365607738494873\n",
      "Loss at step 30500 : 2.9706761837005615\n",
      "Loss at step 31000 : 2.9578957557678223\n",
      "Loss at step 31500 : 3.2464241981506348\n",
      "Loss at step 32000 : 2.776576280593872\n",
      "Loss at step 32500 : 2.924769639968872\n",
      "Loss at step 33000 : 3.2548112869262695\n",
      "Loss at step 33500 : 3.348295211791992\n",
      "Loss at step 34000 : 1.5844569206237793\n",
      "Loss at step 34500 : 2.4853169918060303\n",
      "Loss at step 35000 : 3.0785961151123047\n",
      "Loss at step 35500 : 2.7825090885162354\n",
      "Loss at step 36000 : 3.0044751167297363\n",
      "Loss at step 36500 : 3.8293545246124268\n",
      "Loss at step 37000 : 4.04365348815918\n",
      "Loss at step 37500 : 2.8976330757141113\n",
      "Loss at step 38000 : 2.6480765342712402\n",
      "Loss at step 38500 : 2.6636505126953125\n",
      "Loss at step 39000 : 2.456289529800415\n",
      "Loss at step 39500 : 2.6297075748443604\n",
      "Loss at step 40000 : 2.927323579788208\n",
      "Loss at step 40500 : 2.9063446521759033\n",
      "Loss at step 41000 : 3.249561071395874\n",
      "Loss at step 41500 : 3.4229443073272705\n",
      "Loss at step 42000 : 3.354945182800293\n",
      "Loss at step 42500 : 5.936773777008057\n",
      "Loss at step 43000 : 2.8749520778656006\n",
      "Loss at step 43500 : 3.1078150272369385\n",
      "Loss at step 44000 : 3.8165810108184814\n",
      "Loss at step 44500 : 2.899911880493164\n",
      "Loss at step 45000 : 2.385103940963745\n",
      "Loss at step 45500 : 3.1318068504333496\n",
      "Loss at step 46000 : 2.5800275802612305\n",
      "Loss at step 46500 : 2.7033748626708984\n",
      "Loss at step 47000 : 3.1468820571899414\n",
      "Loss at step 47500 : 3.1564276218414307\n",
      "Loss at step 48000 : 2.2367022037506104\n",
      "Loss at step 48500 : 2.4332916736602783\n",
      "Loss at step 49000 : 3.1757450103759766\n",
      "Loss at step 49500 : 3.0484673976898193\n",
      "Loss at step 50000 : 2.767385244369507\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "loss_vec = []\n",
    "loss_x_vec = []\n",
    "for i in range(generations):\n",
    " batch_inputs, batch_labels = generate_batch_data(text_data, batch_size, window_size)\n",
    " feed_dict = {x_inputs : batch_inputs, y_target : batch_labels}\n",
    " # Run the train step\n",
    " sess.run(optimizer, feed_dict=feed_dict)\n",
    " # Return the loss\n",
    " if (i+1) % print_loss_every == 0:\n",
    "     loss_val = sess.run(loss, feed_dict=feed_dict)\n",
    "     loss_vec.append(loss_val)\n",
    "     loss_x_vec.append(i+1)\n",
    "     print(\"Loss at step {} : {}\".format(i+1, loss_val))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From D:\\software\\anaconda\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-ced79c2f6b84>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel_checkpoint_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"temp/movie_data/\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'cbow_movie_embeddings.ckpt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"embeddings\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0membeddings\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_checkpoint_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"temp/movie_data/\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'movie_vocab.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m   \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_dictionary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\software\\anaconda\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, sess, save_path)\u001b[0m\n\u001b[0;32m   1266\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheckpoint_management\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheckpoint_exists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1267\u001b[0m       raise ValueError(\"The passed save_path is not a valid checkpoint: \"\n\u001b[1;32m-> 1268\u001b[1;33m                        + compat.as_text(save_path))\n\u001b[0m\u001b[0;32m   1269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Restoring parameters from %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The passed save_path is not a valid checkpoint: temp/movie_data/cbow_movie_embeddings.ckpt"
     ],
     "ename": "ValueError",
     "evalue": "The passed save_path is not a valid checkpoint: temp/movie_data/cbow_movie_embeddings.ckpt",
     "output_type": "error"
    }
   ],
   "source": [
    "model_checkpoint_path = os.path.join(\"temp/movie_data/\",'cbow_movie_embeddings.ckpt')\n",
    "saver = tf.train.Saver({\"embeddings\": embeddings})\n",
    "saver.restore(sess, model_checkpoint_path)\n",
    "with open(os.path.join(\"temp/movie_data/\",'movie_vocab.pkl'), 'wb') as f:\n",
    "  pickle.dump(word_dictionary, f)\n",
    "model_checkpoint_path = os.path.join(os.getcwd(),\"temp/movie_data/\",'cbow_movie_embeddings.ckpt')\n",
    "save_path = saver.save(sess, model_checkpoint_path)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}