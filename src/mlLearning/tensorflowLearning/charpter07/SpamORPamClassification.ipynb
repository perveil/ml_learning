{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "import string\n",
    "import requests\n",
    "import io\n",
    "from zipfile import ZipFile\n",
    "from tensorflow.contrib import learn\n",
    "sess = tf.Session()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "zip_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
    "r=requests.get(zip_url)\n",
    "z = ZipFile(io.BytesIO(r.content))\n",
    "file = z.read('SMSSpamCollection')\n",
    "text_data = file.decode()\n",
    "text_data = text_data.encode('ascii',errors='ignore')\n",
    "text_data = text_data.decode().split('\\n')\n",
    "text_data = [x.split('\\t') for x in text_data if len(x)>=1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "texts = [x[1] for x in text_data]\n",
    "target = [x[0] for x in text_data]\n",
    "target = [1 if x=='spam' else 0 for x in target]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "texts=[ x.lower() for x in texts]  #所有word变成小写\n",
    "texts=[''.join(c for c in x if c not  in string.punctuation) for x in texts] #除去所有标点\n",
    "texts=[''.join(c for c in x if c not  in \"0123456789\") for x in texts] \n",
    "texts = [' '.join(x.split()) for x in texts]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-c78c4e367f8e>:3: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From D:\\software\\anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From D:\\software\\anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "sentence_size = 25 #每一个句子所包含的最大单词数\n",
    "min_word_freq = 3\n",
    "vocab_processor=learn.preprocessing.VocabularyProcessor(sentence_size,min_frequency=min_word_freq)\n",
    "vocab_processor.fit_transform(texts)\n",
    "embedding_size=len(vocab_processor.vocabulary_) #2108 表示有2108 个单词"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "train_indices = np.random.choice(len(texts), round(len(texts)*0.8), replace=False)\n",
    "test_indices = np.array(list(set(range(len(texts))) - set(train_indices)))\n",
    "texts_train = [x for ix, x in enumerate(texts) if ix in train_indices]\n",
    "texts_test = [x for ix, x in enumerate(texts) if ix in test_indices]\n",
    "target_train = [x for ix, x in enumerate(target) if ix in train_indices]\n",
    "target_test = [x for ix, x in enumerate(target) if ix in test_indices]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From D:\\software\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "identity_mat = tf.diag(tf.ones(shape=[embedding_size]))\n",
    "#implement a logistic regression\n",
    "A = tf.Variable(tf.random_normal(shape=[embedding_size,1]))  #单词的稀疏向量相加得到最后的向量为2108*1 维\n",
    "b = tf.Variable(tf.random_normal(shape=[1,1]))\n",
    "x_data = tf.placeholder(shape=[sentence_size], dtype=tf.int32)\n",
    "y_target = tf.placeholder(shape=[1, 1], dtype=tf.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "x_embed = tf.nn.embedding_lookup(identity_mat, x_data)\n",
    "x_col_sums = tf.reduce_sum(x_embed, 0)\n",
    "# 模型计算\n",
    "x_col_sums_2D = tf.expand_dims(x_col_sums, 0)\n",
    "model_output = tf.add(tf.matmul(x_col_sums_2D, A), b)\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=model_output, labels=y_target))\n",
    "perdiction=tf.sigmoid(model_output)\n",
    "my_opt=tf.train.GradientDescentOptimizer(0.001)\n",
    "train_step=my_opt.minimize(loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Training Observation #10: Loss = 9.847071e-07\n",
      "Training Observation #20: Loss = 15.392619\n",
      "Training Observation #30: Loss = 5.7623324\n",
      "Training Observation #40: Loss = 10.175142\n",
      "Training Observation #50: Loss = 9.701462\n",
      "Training Observation #60: Loss = 6.3985972\n",
      "Training Observation #70: Loss = 1.2686572\n",
      "Training Observation #80: Loss = 6.8527193\n",
      "Training Observation #90: Loss = 0.012949951\n",
      "Training Observation #100: Loss = 0.83515745\n",
      "Training Observation #110: Loss = 5.6375513\n",
      "Training Observation #120: Loss = 1.1890194\n",
      "Training Observation #130: Loss = 0.1439481\n",
      "Training Observation #140: Loss = 0.008554426\n",
      "Training Observation #150: Loss = 3.2505789\n",
      "Training Observation #160: Loss = 0.0018735728\n",
      "Training Observation #170: Loss = 0.030016717\n",
      "Training Observation #180: Loss = 10.104113\n",
      "Training Observation #190: Loss = 0.0053375377\n",
      "Training Observation #200: Loss = 3.8642669\n",
      "Training Observation #210: Loss = 0.004516901\n",
      "Training Observation #220: Loss = 0.0019680755\n",
      "Training Observation #230: Loss = 0.038145233\n",
      "Training Observation #240: Loss = 0.0034633942\n",
      "Training Observation #250: Loss = 0.0033786246\n",
      "Training Observation #260: Loss = 0.059087373\n",
      "Training Observation #270: Loss = 0.16426657\n",
      "Training Observation #280: Loss = 0.016875952\n",
      "Training Observation #290: Loss = 0.0018128669\n",
      "Training Observation #300: Loss = 0.1349952\n",
      "Training Observation #310: Loss = 0.0018706832\n",
      "Training Observation #320: Loss = 8.913292\n",
      "Training Observation #330: Loss = 0.005350328\n",
      "Training Observation #340: Loss = 0.006776249\n",
      "Training Observation #350: Loss = 0.0002060031\n",
      "Training Observation #360: Loss = 0.00066989544\n",
      "Training Observation #370: Loss = 0.0012018097\n",
      "Training Observation #380: Loss = 1.9101841\n",
      "Training Observation #390: Loss = 0.0024290702\n",
      "Training Observation #400: Loss = 0.0010055363\n",
      "Training Observation #410: Loss = 0.09990351\n",
      "Training Observation #420: Loss = 0.00018183034\n",
      "Training Observation #430: Loss = 2.0540037\n",
      "Training Observation #440: Loss = 0.0010183145\n",
      "Training Observation #450: Loss = 0.03723487\n",
      "Training Observation #460: Loss = 0.0036356084\n",
      "Training Observation #470: Loss = 0.14393964\n",
      "Training Observation #480: Loss = 0.0014132415\n",
      "Training Observation #490: Loss = 0.0006417314\n",
      "Training Observation #500: Loss = 0.15998024\n",
      "Training Observation #510: Loss = 0.016717305\n",
      "Training Observation #520: Loss = 0.015601514\n",
      "Training Observation #530: Loss = 0.06564331\n",
      "Training Observation #540: Loss = 0.86311865\n",
      "Training Observation #550: Loss = 5.89843\n",
      "Training Observation #560: Loss = 0.0071180034\n",
      "Training Observation #570: Loss = 0.00017449466\n",
      "Training Observation #580: Loss = 1.2162776\n",
      "Training Observation #590: Loss = 8.4957075\n",
      "Training Observation #600: Loss = 0.09420127\n",
      "Training Observation #610: Loss = 4.474951\n",
      "Training Observation #620: Loss = 0.025661327\n",
      "Training Observation #630: Loss = 0.005938939\n",
      "Training Observation #640: Loss = 0.00079883053\n",
      "Training Observation #650: Loss = 2.0974085\n",
      "Training Observation #660: Loss = 0.07277989\n",
      "Training Observation #670: Loss = 0.002698586\n",
      "Training Observation #680: Loss = 0.019443095\n",
      "Training Observation #690: Loss = 0.00064641156\n",
      "Training Observation #700: Loss = 0.0317213\n",
      "Training Observation #710: Loss = 0.022996843\n",
      "Training Observation #720: Loss = 0.001999661\n",
      "Training Observation #730: Loss = 0.29898864\n",
      "Training Observation #740: Loss = 0.00020720268\n",
      "Training Observation #750: Loss = 2.0887256\n",
      "Training Observation #760: Loss = 0.0001438004\n",
      "Training Observation #770: Loss = 0.030169865\n",
      "Training Observation #780: Loss = 0.002820911\n",
      "Training Observation #790: Loss = 7.376219e-05\n",
      "Training Observation #800: Loss = 0.007578249\n",
      "Training Observation #810: Loss = 0.0018940419\n",
      "Training Observation #820: Loss = 9.300664\n",
      "Training Observation #830: Loss = 0.0048250575\n",
      "Training Observation #840: Loss = 0.0078272\n",
      "Training Observation #850: Loss = 0.10226016\n",
      "Training Observation #860: Loss = 0.14257416\n",
      "Training Observation #870: Loss = 0.601424\n",
      "Training Observation #880: Loss = 1.6981487e-05\n",
      "Training Observation #890: Loss = 0.00032937646\n",
      "Training Observation #900: Loss = 0.0005915396\n",
      "Training Observation #910: Loss = 3.2377524e-05\n",
      "Training Observation #920: Loss = 0.44887564\n",
      "Training Observation #930: Loss = 0.0027337053\n",
      "Training Observation #940: Loss = 5.664833e-05\n",
      "Training Observation #950: Loss = 7.426919e-05\n",
      "Training Observation #960: Loss = 0.4222857\n",
      "Training Observation #970: Loss = 18.607231\n",
      "Training Observation #980: Loss = 3.1219424e-05\n",
      "Training Observation #990: Loss = 0.001533591\n",
      "Training Observation #1000: Loss = 3.782081e-06\n",
      "Training Observation #1010: Loss = 0.023676062\n",
      "Training Observation #1020: Loss = 3.9746857\n",
      "Training Observation #1030: Loss = 3.5765803\n",
      "Training Observation #1040: Loss = 0.0123665435\n",
      "Training Observation #1050: Loss = 0.000688996\n",
      "Training Observation #1060: Loss = 0.0005728029\n",
      "Training Observation #1070: Loss = 0.0008318983\n",
      "Training Observation #1080: Loss = 7.717727e-05\n",
      "Training Observation #1090: Loss = 6.46709e-05\n",
      "Training Observation #1100: Loss = 7.0045133\n",
      "Training Observation #1110: Loss = 0.0003248008\n",
      "Training Observation #1120: Loss = 0.01898843\n",
      "Training Observation #1130: Loss = 0.42579946\n",
      "Training Observation #1140: Loss = 4.511553\n",
      "Training Observation #1150: Loss = 6.963912e-05\n",
      "Training Observation #1160: Loss = 0.0010457762\n",
      "Training Observation #1170: Loss = 2.1298554\n",
      "Training Observation #1180: Loss = 0.04969621\n",
      "Training Observation #1190: Loss = 6.214015\n",
      "Training Observation #1200: Loss = 5.4159133e-05\n",
      "Training Observation #1210: Loss = 0.00038738793\n",
      "Training Observation #1220: Loss = 0.017750086\n",
      "Training Observation #1230: Loss = 0.00031487326\n",
      "Training Observation #1240: Loss = 0.0023214328\n",
      "Training Observation #1250: Loss = 0.030180326\n",
      "Training Observation #1260: Loss = 0.00014021629\n",
      "Training Observation #1270: Loss = 3.273841\n",
      "Training Observation #1280: Loss = 5.0062294\n",
      "Training Observation #1290: Loss = 8.950572\n",
      "Training Observation #1300: Loss = 0.02203678\n",
      "Training Observation #1310: Loss = 6.8627334e-05\n",
      "Training Observation #1320: Loss = 0.00029104427\n",
      "Training Observation #1330: Loss = 0.001198331\n",
      "Training Observation #1340: Loss = 0.00045520364\n",
      "Training Observation #1350: Loss = 9.724662\n",
      "Training Observation #1360: Loss = 0.00016213623\n",
      "Training Observation #1370: Loss = 1.6825217\n",
      "Training Observation #1380: Loss = 0.0017806253\n",
      "Training Observation #1390: Loss = 0.037820816\n",
      "Training Observation #1400: Loss = 0.0036277398\n",
      "Training Observation #1410: Loss = 0.0012339762\n",
      "Training Observation #1420: Loss = 0.10468177\n",
      "Training Observation #1430: Loss = 0.000152866\n",
      "Training Observation #1440: Loss = 9.828418e-06\n",
      "Training Observation #1450: Loss = 0.14725193\n",
      "Training Observation #1460: Loss = 0.01812372\n",
      "Training Observation #1470: Loss = 0.009005147\n",
      "Training Observation #1480: Loss = 7.7630997\n",
      "Training Observation #1490: Loss = 0.94012654\n",
      "Training Observation #1500: Loss = 0.2016832\n",
      "Training Observation #1510: Loss = 6.7254005e-05\n",
      "Training Observation #1520: Loss = 5.6449767e-06\n",
      "Training Observation #1530: Loss = 4.12069\n",
      "Training Observation #1540: Loss = 0.16527534\n",
      "Training Observation #1550: Loss = 0.00021482007\n",
      "Training Observation #1560: Loss = 6.892314e-05\n",
      "Training Observation #1570: Loss = 9.359624\n",
      "Training Observation #1580: Loss = 5.3014956\n",
      "Training Observation #1590: Loss = 0.00013893069\n",
      "Training Observation #1600: Loss = 0.00011939388\n",
      "Training Observation #1610: Loss = 0.00015938227\n",
      "Training Observation #1620: Loss = 1.8842885\n",
      "Training Observation #1630: Loss = 5.1797284e-05\n",
      "Training Observation #1640: Loss = 0.00013689711\n",
      "Training Observation #1650: Loss = 0.0008395991\n",
      "Training Observation #1660: Loss = 0.00022785582\n",
      "Training Observation #1670: Loss = 5.788824\n",
      "Training Observation #1680: Loss = 0.00031774276\n",
      "Training Observation #1690: Loss = 5.250931e-06\n",
      "Training Observation #1700: Loss = 0.0013857384\n",
      "Training Observation #1710: Loss = 0.0012652583\n",
      "Training Observation #1720: Loss = 0.0009761892\n",
      "Training Observation #1730: Loss = 0.019561037\n",
      "Training Observation #1740: Loss = 0.008490447\n",
      "Training Observation #1750: Loss = 0.0016332993\n",
      "Training Observation #1760: Loss = 0.00035149767\n",
      "Training Observation #1770: Loss = 0.0085080415\n",
      "Training Observation #1780: Loss = 9.278061\n",
      "Training Observation #1790: Loss = 4.3057714\n",
      "Training Observation #1800: Loss = 0.00022569667\n",
      "Training Observation #1810: Loss = 0.006760427\n",
      "Training Observation #1820: Loss = 3.620385e-05\n",
      "Training Observation #1830: Loss = 3.0439231\n",
      "Training Observation #1840: Loss = 4.51673\n",
      "Training Observation #1850: Loss = 0.00033729104\n",
      "Training Observation #1860: Loss = 0.000484205\n",
      "Training Observation #1870: Loss = 0.00016144874\n",
      "Training Observation #1880: Loss = 0.00096357183\n",
      "Training Observation #1890: Loss = 4.6774387\n",
      "Training Observation #1900: Loss = 0.00044005548\n",
      "Training Observation #1910: Loss = 0.00015113836\n",
      "Training Observation #1920: Loss = 0.0074492106\n",
      "Training Observation #1930: Loss = 4.004844\n",
      "Training Observation #1940: Loss = 10.047186\n",
      "Training Observation #1950: Loss = 0.0060043633\n",
      "Training Observation #1960: Loss = 0.25934115\n",
      "Training Observation #1970: Loss = 0.00994085\n",
      "Training Observation #1980: Loss = 2.7271491e-05\n",
      "Training Observation #1990: Loss = 0.055959027\n",
      "Training Observation #2000: Loss = 0.000401337\n",
      "Training Observation #2010: Loss = 1.0612508\n",
      "Training Observation #2020: Loss = 1.5948575e-05\n",
      "Training Observation #2030: Loss = 5.936367\n",
      "Training Observation #2040: Loss = 0.00012733554\n",
      "Training Observation #2050: Loss = 0.1852895\n",
      "Training Observation #2060: Loss = 3.813037e-05\n",
      "Training Observation #2070: Loss = 0.07215543\n",
      "Training Observation #2080: Loss = 14.266626\n",
      "Training Observation #2090: Loss = 0.30059263\n",
      "Training Observation #2100: Loss = 0.0015801848\n",
      "Training Observation #2110: Loss = 0.00036299793\n",
      "Training Observation #2120: Loss = 0.00057232246\n",
      "Training Observation #2130: Loss = 0.02466113\n",
      "Training Observation #2140: Loss = 0.018262012\n",
      "Training Observation #2150: Loss = 0.0023493166\n",
      "Training Observation #2160: Loss = 0.2122689\n",
      "Training Observation #2170: Loss = 4.079566e-05\n",
      "Training Observation #2180: Loss = 0.042666577\n",
      "Training Observation #2190: Loss = 0.0043458324\n",
      "Training Observation #2200: Loss = 4.6959117e-06\n",
      "Training Observation #2210: Loss = 6.701361e-05\n",
      "Training Observation #2220: Loss = 0.080633216\n",
      "Training Observation #2230: Loss = 0.006882518\n",
      "Training Observation #2240: Loss = 0.0008638656\n",
      "Training Observation #2250: Loss = 0.16250248\n",
      "Training Observation #2260: Loss = 0.0023307877\n",
      "Training Observation #2270: Loss = 0.001155352\n",
      "Training Observation #2280: Loss = 3.4632335\n",
      "Training Observation #2290: Loss = 0.06343804\n",
      "Training Observation #2300: Loss = 2.7541122\n",
      "Training Observation #2310: Loss = 0.0036632249\n",
      "Training Observation #2320: Loss = 0.19680513\n",
      "Training Observation #2330: Loss = 0.00083248457\n",
      "Training Observation #2340: Loss = 0.00020464938\n",
      "Training Observation #2350: Loss = 0.0003056997\n",
      "Training Observation #2360: Loss = 4.170378\n",
      "Training Observation #2370: Loss = 0.0010008368\n",
      "Training Observation #2380: Loss = 0.0002329548\n",
      "Training Observation #2390: Loss = 5.076996\n",
      "Training Observation #2400: Loss = 0.0002858134\n",
      "Training Observation #2410: Loss = 0.0016749996\n",
      "Training Observation #2420: Loss = 0.4521592\n",
      "Training Observation #2430: Loss = 0.00013015566\n",
      "Training Observation #2440: Loss = 0.0059974054\n",
      "Training Observation #2450: Loss = 0.00753983\n",
      "Training Observation #2460: Loss = 2.464722e-05\n",
      "Training Observation #2470: Loss = 0.0003931433\n",
      "Training Observation #2480: Loss = 6.612466e-05\n",
      "Training Observation #2490: Loss = 0.03985784\n",
      "Training Observation #2500: Loss = 0.05479683\n",
      "Training Observation #2510: Loss = 0.12497422\n",
      "Training Observation #2520: Loss = 0.1841939\n",
      "Training Observation #2530: Loss = 0.09966421\n",
      "Training Observation #2540: Loss = 6.1530096e-05\n",
      "Training Observation #2550: Loss = 7.514371\n",
      "Training Observation #2560: Loss = 0.00034064302\n",
      "Training Observation #2570: Loss = 0.0009771589\n",
      "Training Observation #2580: Loss = 0.021018412\n",
      "Training Observation #2590: Loss = 8.289236\n",
      "Training Observation #2600: Loss = 2.9533594\n",
      "Training Observation #2610: Loss = 0.003552792\n",
      "Training Observation #2620: Loss = 0.0025679867\n",
      "Training Observation #2630: Loss = 0.041995917\n",
      "Training Observation #2640: Loss = 0.0039342036\n",
      "Training Observation #2650: Loss = 7.0064883\n",
      "Training Observation #2660: Loss = 1.16305\n",
      "Training Observation #2670: Loss = 7.7155366\n",
      "Training Observation #2680: Loss = 1.2755735e-06\n",
      "Training Observation #2690: Loss = 8.5117594e-05\n",
      "Training Observation #2700: Loss = 0.00014328271\n",
      "Training Observation #2710: Loss = 4.7551227\n",
      "Training Observation #2720: Loss = 9.849643e-05\n",
      "Training Observation #2730: Loss = 0.08996916\n",
      "Training Observation #2740: Loss = 1.0895518e-05\n",
      "Training Observation #2750: Loss = 0.35331133\n",
      "Training Observation #2760: Loss = 0.0037206244\n",
      "Training Observation #2770: Loss = 0.0007169775\n",
      "Training Observation #2780: Loss = 0.00039152705\n",
      "Training Observation #2790: Loss = 0.001452749\n",
      "Training Observation #2800: Loss = 6.478563\n",
      "Training Observation #2810: Loss = 0.08900975\n",
      "Training Observation #2820: Loss = 0.031514682\n",
      "Training Observation #2830: Loss = 0.00040877034\n",
      "Training Observation #2840: Loss = 0.009013569\n",
      "Training Observation #2850: Loss = 0.00082274707\n",
      "Training Observation #2860: Loss = 0.0021605622\n",
      "Training Observation #2870: Loss = 0.004961768\n",
      "Training Observation #2880: Loss = 0.0006661735\n",
      "Training Observation #2890: Loss = 0.009013668\n",
      "Training Observation #2900: Loss = 3.666132e-05\n",
      "Training Observation #2910: Loss = 0.0064061456\n",
      "Training Observation #2920: Loss = 0.2589576\n",
      "Training Observation #2930: Loss = 0.018170914\n",
      "Training Observation #2940: Loss = 0.03518895\n",
      "Training Observation #2950: Loss = 0.00033926457\n",
      "Training Observation #2960: Loss = 0.0006040374\n",
      "Training Observation #2970: Loss = 2.5630996e-05\n",
      "Training Observation #2980: Loss = 0.0003772561\n",
      "Training Observation #2990: Loss = 7.630413e-05\n",
      "Training Observation #3000: Loss = 2.3968898e-05\n",
      "Training Observation #3010: Loss = 0.005624893\n",
      "Training Observation #3020: Loss = 0.12777428\n",
      "Training Observation #3030: Loss = 0.005827498\n",
      "Training Observation #3040: Loss = 2.8382382\n",
      "Training Observation #3050: Loss = 0.01822864\n",
      "Training Observation #3060: Loss = 3.0122936\n",
      "Training Observation #3070: Loss = 0.00033486824\n",
      "Training Observation #3080: Loss = 0.08090486\n",
      "Training Observation #3090: Loss = 6.452704\n",
      "Training Observation #3100: Loss = 0.0016590151\n",
      "Training Observation #3110: Loss = 5.2957025\n",
      "Training Observation #3120: Loss = 9.148412\n",
      "Training Observation #3130: Loss = 0.00026261105\n",
      "Training Observation #3140: Loss = 9.519627\n",
      "Training Observation #3150: Loss = 0.041391406\n",
      "Training Observation #3160: Loss = 7.7351937\n",
      "Training Observation #3170: Loss = 0.019759895\n",
      "Training Observation #3180: Loss = 8.416285\n",
      "Training Observation #3190: Loss = 0.00032743777\n",
      "Training Observation #3200: Loss = 0.00062171754\n",
      "Training Observation #3210: Loss = 0.0130902575\n",
      "Training Observation #3220: Loss = 6.581032\n",
      "Training Observation #3230: Loss = 0.17691135\n",
      "Training Observation #3240: Loss = 0.17150632\n",
      "Training Observation #3250: Loss = 0.0023755599\n",
      "Training Observation #3260: Loss = 0.011638258\n",
      "Training Observation #3270: Loss = 0.0014466412\n",
      "Training Observation #3280: Loss = 0.059062794\n",
      "Training Observation #3290: Loss = 0.7308059\n",
      "Training Observation #3300: Loss = 0.021845177\n",
      "Training Observation #3310: Loss = 0.00021627775\n",
      "Training Observation #3320: Loss = 0.0034968415\n",
      "Training Observation #3330: Loss = 0.3979124\n",
      "Training Observation #3340: Loss = 1.4520926\n",
      "Training Observation #3350: Loss = 0.33532095\n",
      "Training Observation #3360: Loss = 6.124947\n",
      "Training Observation #3370: Loss = 0.0058964733\n",
      "Training Observation #3380: Loss = 0.0350101\n",
      "Training Observation #3390: Loss = 0.20273004\n",
      "Training Observation #3400: Loss = 0.13654931\n",
      "Training Observation #3410: Loss = 4.9746895\n",
      "Training Observation #3420: Loss = 7.004717\n",
      "Training Observation #3430: Loss = 0.001181742\n",
      "Training Observation #3440: Loss = 0.00026163177\n",
      "Training Observation #3450: Loss = 4.320439\n",
      "Training Observation #3460: Loss = 0.07108203\n",
      "Training Observation #3470: Loss = 7.9768457\n",
      "Training Observation #3480: Loss = 0.06274852\n",
      "Training Observation #3490: Loss = 0.03007821\n",
      "Training Observation #3500: Loss = 0.001964971\n",
      "Training Observation #3510: Loss = 4.9731374e-05\n",
      "Training Observation #3520: Loss = 5.8609457\n",
      "Training Observation #3530: Loss = 0.008570603\n",
      "Training Observation #3540: Loss = 0.00037095306\n",
      "Training Observation #3550: Loss = 0.0004015873\n",
      "Training Observation #3560: Loss = 0.00011166933\n",
      "Training Observation #3570: Loss = 4.0562387\n",
      "Training Observation #3580: Loss = 0.45925868\n",
      "Training Observation #3590: Loss = 0.018386967\n",
      "Training Observation #3600: Loss = 0.0006012333\n",
      "Training Observation #3610: Loss = 0.00033031526\n",
      "Training Observation #3620: Loss = 0.0006776468\n",
      "Training Observation #3630: Loss = 0.037695263\n",
      "Training Observation #3640: Loss = 8.321981\n",
      "Training Observation #3650: Loss = 1.6414237\n",
      "Training Observation #3660: Loss = 0.0060084043\n",
      "Training Observation #3670: Loss = 0.0013409011\n",
      "Training Observation #3680: Loss = 0.028939396\n",
      "Training Observation #3690: Loss = 0.00019257862\n",
      "Training Observation #3700: Loss = 0.0020950874\n",
      "Training Observation #3710: Loss = 0.0010886341\n",
      "Training Observation #3720: Loss = 0.0046776086\n",
      "Training Observation #3730: Loss = 0.0028632695\n",
      "Training Observation #3740: Loss = 0.079143874\n",
      "Training Observation #3750: Loss = 0.051427815\n",
      "Training Observation #3760: Loss = 0.0044904524\n",
      "Training Observation #3770: Loss = 0.0039563575\n",
      "Training Observation #3780: Loss = 0.00052200374\n",
      "Training Observation #3790: Loss = 0.0023903467\n",
      "Training Observation #3800: Loss = 0.00023812038\n",
      "Training Observation #3810: Loss = 2.5470717\n",
      "Training Observation #3820: Loss = 0.005246301\n",
      "Training Observation #3830: Loss = 0.8468663\n",
      "Training Observation #3840: Loss = 6.1483755e-05\n",
      "Training Observation #3850: Loss = 2.5925929e-05\n",
      "Training Observation #3860: Loss = 0.004675705\n",
      "Training Observation #3870: Loss = 0.5397937\n",
      "Training Observation #3880: Loss = 1.1291107\n",
      "Training Observation #3890: Loss = 0.0006244653\n",
      "Training Observation #3900: Loss = 8.20278\n",
      "Training Observation #3910: Loss = 1.7339177\n",
      "Training Observation #3920: Loss = 0.00048151758\n",
      "Training Observation #3930: Loss = 0.018506823\n",
      "Training Observation #3940: Loss = 0.5799196\n",
      "Training Observation #3950: Loss = 5.202387e-05\n",
      "Training Observation #3960: Loss = 0.00016657785\n",
      "Training Observation #3970: Loss = 6.9254995e-05\n",
      "Training Observation #3980: Loss = 0.00022504397\n",
      "Training Observation #3990: Loss = 4.8328648\n",
      "Training Observation #4000: Loss = 6.3327084\n",
      "Training Observation #4010: Loss = 0.0021408985\n",
      "Training Observation #4020: Loss = 10.4732485\n",
      "Training Observation #4030: Loss = 0.00075892\n",
      "Training Observation #4040: Loss = 0.00097107416\n",
      "Training Observation #4050: Loss = 0.00016978756\n",
      "Training Observation #4060: Loss = 0.03924557\n",
      "Training Observation #4070: Loss = 0.00014378285\n",
      "Training Observation #4080: Loss = 0.00026749898\n",
      "Training Observation #4090: Loss = 0.0016437719\n",
      "Training Observation #4100: Loss = 0.018208474\n",
      "Training Observation #4110: Loss = 0.0002453921\n",
      "Training Observation #4120: Loss = 0.00017922456\n",
      "Training Observation #4130: Loss = 0.0012528207\n",
      "Training Observation #4140: Loss = 0.0003023621\n",
      "Training Observation #4150: Loss = 5.8284177e-06\n",
      "Training Observation #4160: Loss = 0.43966725\n",
      "Training Observation #4170: Loss = 6.428869\n",
      "Training Observation #4180: Loss = 4.3425327e-05\n",
      "Training Observation #4190: Loss = 0.0001468778\n",
      "Training Observation #4200: Loss = 1.7583464\n",
      "Training Observation #4210: Loss = 0.012068753\n",
      "Training Observation #4220: Loss = 0.0033784062\n",
      "Training Observation #4230: Loss = 7.555773e-05\n",
      "Training Observation #4240: Loss = 0.0028376149\n",
      "Training Observation #4250: Loss = 6.315134\n",
      "Training Observation #4260: Loss = 0.00037156316\n",
      "Training Observation #4270: Loss = 0.0044348855\n",
      "Training Observation #4280: Loss = 0.044731203\n",
      "Training Observation #4290: Loss = 0.0002646031\n",
      "Training Observation #4300: Loss = 0.0008591042\n",
      "Training Observation #4310: Loss = 0.2268604\n",
      "Training Observation #4320: Loss = 0.002273076\n",
      "Training Observation #4330: Loss = 0.0004177391\n",
      "Training Observation #4340: Loss = 0.10179042\n",
      "Training Observation #4350: Loss = 0.00043517\n",
      "Training Observation #4360: Loss = 0.00018830714\n",
      "Training Observation #4370: Loss = 0.0021963988\n",
      "Training Observation #4380: Loss = 1.3662424\n",
      "Training Observation #4390: Loss = 0.016735574\n",
      "Training Observation #4400: Loss = 0.0006797322\n",
      "Training Observation #4410: Loss = 0.00018450036\n",
      "Training Observation #4420: Loss = 0.00035704084\n",
      "Training Observation #4430: Loss = 4.1338625\n",
      "Training Observation #4440: Loss = 0.0073195933\n",
      "Training Observation #4450: Loss = 0.10873456\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "loss_vec = []\n",
    "train_acc_all = []\n",
    "train_acc_avg = []\n",
    "# ix 为texts_train的中的每一个sentence的index， t 为texts_train的中的每一个sentence的每一个word对应vocabulary的index\n",
    "for ix,t in enumerate(vocab_processor.fit_transform(texts_train)):\n",
    "    y_data = [[target_train[ix]]]\n",
    "    sess.run(train_step, feed_dict={x_data: t, y_target: y_data})\n",
    "    temp_loss = sess.run(loss, feed_dict={x_data: t, y_target: y_data})\n",
    "    loss_vec.append(temp_loss)\n",
    "    if (ix+1)%100==0:\n",
    "        print('Training Observation #' + str(ix+1) + ': Loss = ' + \n",
    "        str(temp_loss))\n",
    "    [[temp_pred]] = sess.run(perdiction, feed_dict={x_data:t, y_target:y_data})\n",
    "    train_acc_temp = target_train[ix]==np.round(temp_pred)\n",
    "    train_acc_all.append(train_acc_temp)\n",
    "    if len(train_acc_all) >= 50:\n",
    "     train_acc_avg.append(np.mean(train_acc_all[-50:]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "0.84\n",
      "0.768609865470852\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(train_acc_avg[-1]) # 84% train\n",
    "test_acc_all=[]\n",
    "for ix,t in enumerate(vocab_processor.fit_transform(texts_test)):\n",
    "    y_data=[[target_test[ix]]]\n",
    "    [[temp_pred]] = sess.run(perdiction, feed_dict={x_data:t, y_target:y_data})\n",
    "    test_acc_temp = target_test[ix]==np.round(temp_pred)\n",
    "    test_acc_all.append(test_acc_temp)\n",
    "print(np.mean(test_acc_all)) #78%"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}